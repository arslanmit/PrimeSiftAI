{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNhRGCv6jim+ockcQyTuv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arslanmit/PrimeSiftAI/blob/20250223v001/AI_Core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6PHn5SeQlOJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Quietly install needed packages\n",
        "# ================================\n",
        "!pip install --quiet ipywidgets xgboost scikit-optimize tqdm catboost\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*'force_all_finite' was renamed to 'ensure_all_finite'.*\")\n",
        "\n",
        "import io\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler,\n",
        "    Normalizer, QuantileTransformer, PowerTransformer\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Discriminant Analysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "# Other classifiers\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, DotProduct\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, ExtraTreesClassifier,\n",
        "    AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        ")\n",
        "import xgboost\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "###############################################################################\n",
        "# Tiny embedded data (10 rows each)\n",
        "###############################################################################\n",
        "HEART_DISEASE_CSV_DATA = \"\"\"age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,target\n",
        "63,1,0,145,233,1,2,150,0,2.3,2,0,2,0\n",
        "67,1,3,160,286,0,2,108,1,1.5,1,3,1,1\n",
        "37,1,2,130,250,0,0,187,0,3.5,2,0,1,0\n",
        "41,0,1,130,204,0,2,172,0,1.4,0,0,1,0\n",
        "56,1,1,120,236,0,0,178,0,0.8,0,0,1,0\n",
        "62,0,3,140,268,0,2,160,0,3.6,2,2,1,1\n",
        "57,0,3,120,354,0,0,163,1,0.6,0,0,1,0\n",
        "63,1,3,130,254,0,2,147,0,1.4,1,1,3,1\n",
        "67,1,3,120,229,0,2,129,1,2.6,1,2,3,1\n",
        "59,1,3,140,241,0,0,123,1,0.2,1,0,3,1\n",
        "\"\"\"\n",
        "\n",
        "BREAST_CANCER_CSV_DATA = \"\"\"target,radius_mean,texture_mean,perimeter_mean,area_mean,smoothness_mean,compactness_mean,concavity_mean,concave points_mean,symmetry_mean,fractal_dimension_mean\n",
        "1,17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871\n",
        "1,20.57,17.77,132.9,1326,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667\n",
        "1,19.69,21.25,130,1203,0.1096,0.1599,0.1974,0.1279,0.2069,0.05999\n",
        "1,11.42,20.38,77.58,386.1,0.1425,0.2839,0.2414,0.1052,0.2597,0.09744\n",
        "1,20.29,14.34,135.1,1297,0.1003,0.1328,0.198,0.1043,0.1809,0.05883\n",
        "1,12.45,15.7,82.57,477.1,0.1278,0.17,0.1578,0.08089,0.2087,0.07613\n",
        "1,18.25,19.98,119.6,1040,0.09463,0.109,0.1127,0.074,0.1794,0.05742\n",
        "1,13.71,20.83,90.2,577.9,0.1189,0.1645,0.09366,0.05985,0.2196,0.07451\n",
        "1,13,21.82,87.5,519.8,0.1273,0.1932,0.1859,0.09353,0.235,0.07389\n",
        "1,12.46,24.04,83.97,475.9,0.1186,0.2396,0.2273,0.08543,0.203,0.08243\n",
        "\"\"\"\n",
        "\n",
        "def load_file_data(file_path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"CSV file '{file_path}' not found.\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "###############################################################################\n",
        "# Full manual profiling\n",
        "###############################################################################\n",
        "def custom_full_profiling(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    target_present = ('target' in df.columns)\n",
        "    if target_present and pd.api.types.is_numeric_dtype(df['target']):\n",
        "        correlations = df.corrwith(df['target'])\n",
        "    else:\n",
        "        correlations = pd.Series(dtype='float64')\n",
        "    rows = []\n",
        "    for col in df.columns:\n",
        "        dtype = df[col].dtype\n",
        "        non_null = df[col].notnull().sum()\n",
        "        missing = df[col].isnull().sum()\n",
        "        uniq = df[col].nunique(dropna=False)\n",
        "        info_dict = {\n",
        "            'Column': col,\n",
        "            'Dtype': str(dtype),\n",
        "            '#Non-Null': non_null,\n",
        "            '#Missing': missing,\n",
        "            '#Unique': uniq\n",
        "        }\n",
        "        if pd.api.types.is_numeric_dtype(dtype):\n",
        "            info_dict['Min'] = df[col].min(skipna=True)\n",
        "            info_dict['Max'] = df[col].max(skipna=True)\n",
        "            info_dict['Mean'] = df[col].mean(skipna=True)\n",
        "            info_dict['Median'] = df[col].median(skipna=True)\n",
        "            info_dict['Std'] = df[col].std(skipna=True)\n",
        "            info_dict['Corr(target)'] = correlations.get(col, None)\n",
        "            info_dict['Top'] = None\n",
        "            info_dict['Freq'] = None\n",
        "        else:\n",
        "            info_dict['Min'] = None\n",
        "            info_dict['Max'] = None\n",
        "            info_dict['Mean'] = None\n",
        "            info_dict['Median'] = None\n",
        "            info_dict['Std'] = None\n",
        "            info_dict['Corr(target)'] = None\n",
        "            top_val_series = df[col].value_counts(dropna=False)\n",
        "            if len(top_val_series) > 0:\n",
        "                info_dict['Top'] = top_val_series.index[0]\n",
        "                info_dict['Freq'] = top_val_series.iloc[0]\n",
        "            else:\n",
        "                info_dict['Top'] = None\n",
        "                info_dict['Freq'] = None\n",
        "        rows.append(info_dict)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def dynamic_explore_data(df: pd.DataFrame, max_cols: int):\n",
        "    prof_df = custom_full_profiling(df)\n",
        "    print(\"=== Full Profiling Info ===\")\n",
        "    display(prof_df)\n",
        "    total_missing = df.isnull().sum().sum()\n",
        "    if total_missing == 0:\n",
        "        print(\"No missing values (above).\")\n",
        "    else:\n",
        "        print(f\"Missing values found: {total_missing} (above).\")\n",
        "    if \"target\" not in df.columns:\n",
        "        print(\"\\nNo 'target' => skipping pairplot.\")\n",
        "        return\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if \"target\" in numeric_cols:\n",
        "        numeric_cols.remove(\"target\")\n",
        "    use_cols = numeric_cols[:max_cols]\n",
        "    if not use_cols:\n",
        "        print(\"\\nNo numeric columns => skipping pairplot.\")\n",
        "        return\n",
        "    pcols = use_cols + [\"target\"]\n",
        "    if not set(pcols).issubset(df.columns):\n",
        "        print(\"\\nSome pairplot columns not found => skipping.\")\n",
        "        return\n",
        "    print(\"\\nPlotting Pairplot (may take time):\")\n",
        "    with tqdm(total=1, desc=\"Pairplot\") as pbar:\n",
        "        g = sns.pairplot(df[pcols], hue=\"target\", palette=\"Greys\",\n",
        "                         markers=[\"o\", \"D\"], plot_kws={\"s\":25, \"alpha\":0.75}, height=3)\n",
        "        g.fig.suptitle(f\"Pairplot of up to {max_cols} Numeric + 'target'\", y=1.02)\n",
        "        plt.show()\n",
        "        pbar.update(1)\n",
        "\n",
        "###############################################################################\n",
        "# Define default models\n",
        "###############################################################################\n",
        "stacking_default = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"lr\", LogisticRegression(max_iter=1000, random_state=42)),\n",
        "        (\"knn\", KNeighborsClassifier())\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(random_state=42)\n",
        ")\n",
        "gpc_default = GaussianProcessClassifier(kernel=1.0 * RBF(1.0), random_state=42)\n",
        "\n",
        "def create_pipeline(clf, scaling_method: str) -> Pipeline:\n",
        "    steps = [(\"selector\", SelectKBest(score_func=f_classif, k=\"all\"))]\n",
        "    def get_scaler(method):\n",
        "        scalers = {\n",
        "            \"StandardScaler\": StandardScaler(),\n",
        "            \"MinMaxScaler\": MinMaxScaler(),\n",
        "            \"RobustScaler\": RobustScaler(),\n",
        "            \"MaxAbsScaler\": MaxAbsScaler(),\n",
        "            \"Normalizer\": Normalizer(),\n",
        "            \"QuantileTransformer\": QuantileTransformer(output_distribution=\"normal\"),\n",
        "            \"PowerTransformer\": PowerTransformer(),\n",
        "            \"none\": None\n",
        "        }\n",
        "        return scalers.get(method, StandardScaler())\n",
        "    s = get_scaler(scaling_method)\n",
        "    if s is not None:\n",
        "        steps.append((\"scaler\", s))\n",
        "    steps.append((\"clf\", clf))\n",
        "    return Pipeline(steps)\n",
        "\n",
        "def define_all_models(scaling_method: str) -> dict:\n",
        "    qda_default = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "    return {\n",
        "        \"Naive Bayes\": create_pipeline(GaussianNB(), scaling_method),\n",
        "        \"k-nearest neighbors algorithm\": create_pipeline(KNeighborsClassifier(), scaling_method),\n",
        "        \"Logistic Regression\": create_pipeline(LogisticRegression(max_iter=1000, random_state=42), scaling_method),\n",
        "        \"Ridge Classifier\": create_pipeline(RidgeClassifier(random_state=42), scaling_method),\n",
        "        \"Linear Discriminant Analysis\": create_pipeline(LinearDiscriminantAnalysis(), scaling_method),\n",
        "        \"Quadratic Discriminant Analysis\": create_pipeline(qda_default, scaling_method),\n",
        "        \"Decision Tree\": create_pipeline(DecisionTreeClassifier(random_state=42), scaling_method),\n",
        "        \"Support vector machine\": create_pipeline(SVC(probability=True, random_state=42), scaling_method),\n",
        "        \"Random Forest\": create_pipeline(RandomForestClassifier(random_state=42), scaling_method),\n",
        "        \"Extra Trees\": create_pipeline(ExtraTreesClassifier(random_state=42), scaling_method),\n",
        "        \"AdaBoost\": create_pipeline(AdaBoostClassifier(random_state=42), scaling_method),\n",
        "        \"Gradient Boosting\": create_pipeline(GradientBoostingClassifier(random_state=42), scaling_method),\n",
        "        \"LightGBM\": create_pipeline(lgb.LGBMClassifier(random_state=42, verbose=-1, force_col_wise=True), scaling_method),\n",
        "        \"eXtreme Gradient Boosting\": create_pipeline(xgboost.XGBClassifier(eval_metric=\"logloss\", random_state=42), scaling_method),\n",
        "        \"CatBoost\": create_pipeline(CatBoostClassifier(verbose=0, random_state=42), scaling_method),\n",
        "        \"Stacked Generalization\": create_pipeline(stacking_default, scaling_method),\n",
        "        \"Gaussian Process\": create_pipeline(gpc_default, scaling_method),\n",
        "    }\n",
        "\n",
        "def evaluate_model(model, X_test, y_test) -> dict:\n",
        "    y_pred = model.predict(X_test)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        auc_ = roc_auc_score(y_test, y_prob)\n",
        "    else:\n",
        "        if hasattr(model, \"decision_function\"):\n",
        "            dec = model.decision_function(X_test)\n",
        "            dmin = dec.min()\n",
        "            dmax = dec.max()\n",
        "            y_prob = (dec - dmin) / (dmax - dmin) if dmax != dmin else np.zeros_like(dec)\n",
        "            auc_ = roc_auc_score(y_test, y_prob)\n",
        "        else:\n",
        "            auc_ = -1.0\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"AUC\": auc_\n",
        "    }\n",
        "\n",
        "from sklearn.gaussian_process.kernels import DotProduct\n",
        "def define_param_grids() -> dict:\n",
        "    \"\"\"\n",
        "    Expanded hyperparameter grids:\n",
        "      - Ridge Classifier: tune alpha and solver.\n",
        "      - Linear Discriminant Analysis: tune solver (lsqr, eigen) and shrinkage (None, auto).\n",
        "      - LightGBM: add max_depth.\n",
        "      - Stacked Generalization: tune final_estimator__C.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"Naive Bayes\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
        "        },\n",
        "        \"k-nearest neighbors algorithm\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_neighbors\": [3, 5, 7],\n",
        "            \"clf__weights\": [\"uniform\", \"distance\"]\n",
        "        },\n",
        "        \"Logistic Regression\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__C\": [0.01, 0.1, 1, 10],\n",
        "            \"clf__penalty\": [\"l2\"]\n",
        "        },\n",
        "        \"Ridge Classifier\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__alpha\": [0.01, 0.1, 1.0, 10.0],\n",
        "            \"clf__solver\": [\"auto\", \"sag\"]\n",
        "        },\n",
        "        \"Linear Discriminant Analysis\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__solver\": [\"lsqr\", \"eigen\"],\n",
        "            \"clf__shrinkage\": [None, \"auto\"]\n",
        "        },\n",
        "        \"Quadratic Discriminant Analysis\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__reg_param\": [0.1, 0.2]\n",
        "        },\n",
        "        \"Decision Tree\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__max_depth\": [None, 3, 5, 10],\n",
        "            \"clf__min_samples_split\": [2, 5, 10]\n",
        "        },\n",
        "        \"Support vector machine\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__C\": [0.1, 1, 10],\n",
        "            \"clf__kernel\": [\"linear\", \"rbf\"],\n",
        "            \"clf__gamma\": [\"scale\"]\n",
        "        },\n",
        "        \"Random Forest\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 200],\n",
        "            \"clf__max_depth\": [None, 5, 10]\n",
        "        },\n",
        "        \"Extra Trees\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 150, 200],\n",
        "            \"clf__max_depth\": [3, 5, 7]\n",
        "        },\n",
        "        \"AdaBoost\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 200],\n",
        "            \"clf__learning_rate\": [0.01, 0.1, 1]\n",
        "        },\n",
        "        \"Gradient Boosting\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 200],\n",
        "            \"clf__learning_rate\": [0.01, 0.1, 1],\n",
        "            \"clf__max_depth\": [3, 5, 7]\n",
        "        },\n",
        "        \"LightGBM\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 200],\n",
        "            \"clf__learning_rate\": [0.01, 0.1, 0.2],\n",
        "            \"clf__num_leaves\": [20, 31, 50],\n",
        "            \"clf__max_depth\": [-1, 3, 5]\n",
        "        },\n",
        "        \"eXtreme Gradient Boosting\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__n_estimators\": [50, 100, 200],\n",
        "            \"clf__learning_rate\": [0.01, 0.1, 0.2],\n",
        "            \"clf__max_depth\": [3, 5, 7]\n",
        "        },\n",
        "        \"CatBoost\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__iterations\": [50, 100, 200],\n",
        "            \"clf__learning_rate\": [0.01, 0.1],\n",
        "            \"clf__depth\": [3, 5, 7]\n",
        "        },\n",
        "        \"Stacked Generalization\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__final_estimator__C\": [0.01, 0.1, 1, 10]\n",
        "        },\n",
        "        \"Gaussian Process\": {\n",
        "            \"selector__k\": [5, \"all\"],\n",
        "            \"clf__kernel\": [1.0 * RBF(1.0), DotProduct()]\n",
        "        }\n",
        "    }\n",
        "\n",
        "def tune_models(models, param_grids, X_train, y_train, X_test, y_test, search_method: str, show_tuning_log: bool):\n",
        "    tuned_models = {}\n",
        "    tuned_results = {}\n",
        "    best_params = {}\n",
        "    with tqdm(total=len(models), desc=\"Hyperparameter Tuning\") as pbar:\n",
        "        for mname, pipeline in models.items():\n",
        "            if show_tuning_log:\n",
        "                print(f\"Tuning {mname}...\")\n",
        "            grid_params = param_grids.get(mname, {})\n",
        "            if search_method == \"none\" or not grid_params:\n",
        "                pipeline.fit(X_train, y_train)\n",
        "                tuned_models[mname] = pipeline\n",
        "                result = evaluate_model(pipeline, X_test, y_test)\n",
        "                tuned_results[mname] = result\n",
        "                best_params[mname] = None\n",
        "                print(f\" {mname} (Tuned): AUC={result['AUC']:.4f}\")\n",
        "                if show_tuning_log:\n",
        "                    print(f\"   No tuning for {mname}. Using default.\\n\")\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "            if search_method == \"grid\":\n",
        "                search_cv = GridSearchCV(pipeline, grid_params, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
        "            elif search_method == \"random\":\n",
        "                search_cv = RandomizedSearchCV(\n",
        "                    pipeline, grid_params, cv=3, scoring=\"roc_auc\",\n",
        "                    n_iter=10, n_jobs=-1, verbose=0, random_state=42\n",
        "                )\n",
        "            else:  # bayesian\n",
        "                search_cv = BayesSearchCV(\n",
        "                    pipeline, grid_params, cv=3, scoring=\"roc_auc\",\n",
        "                    n_iter=10, n_jobs=-1, random_state=42\n",
        "                )\n",
        "            search_cv.fit(X_train, y_train)\n",
        "            best_model = search_cv.best_estimator_\n",
        "            tuned_models[mname] = best_model\n",
        "            result = evaluate_model(best_model, X_test, y_test)\n",
        "            tuned_results[mname] = result\n",
        "            best_params[mname] = search_cv.best_params_\n",
        "            print(f\" {mname} (Tuned): AUC={result['AUC']:.4f}\")\n",
        "            if show_tuning_log:\n",
        "                print(f\"   [{search_method.capitalize()}] Best CV AUC: {search_cv.best_score_:.4f} | Best Params: {search_cv.best_params_}\\n\")\n",
        "            pbar.update(1)\n",
        "    return tuned_models, tuned_results, best_params\n",
        "\n",
        "def build_comparison_table(default_results, tuned_results, search_method, scaling_method):\n",
        "    comp_dict = {}\n",
        "    for mname in default_results:\n",
        "        comp_dict[mname] = {\n",
        "            \"Accuracy (Default)\":  default_results[mname][\"Accuracy\"],\n",
        "            \"Accuracy (Tuned)\":    tuned_results[mname][\"Accuracy\"],\n",
        "            \"Precision (Default)\": default_results[mname][\"Precision\"],\n",
        "            \"Precision (Tuned)\":   tuned_results[mname][\"Precision\"],\n",
        "            \"Recall (Default)\":    default_results[mname][\"Recall\"],\n",
        "            \"Recall (Tuned)\":      tuned_results[mname][\"Recall\"],\n",
        "            \"F1 Score (Default)\":  default_results[mname][\"F1 Score\"],\n",
        "            \"F1 Score (Tuned)\":    tuned_results[mname][\"F1 Score\"],\n",
        "            \"AUC (Default)\":       default_results[mname][\"AUC\"],\n",
        "            \"AUC (Tuned)\":         tuned_results[mname][\"AUC\"]\n",
        "        }\n",
        "    df_comp = pd.DataFrame(comp_dict).T\n",
        "    df_comp[\"Selected search method\"] = search_method\n",
        "    df_comp[\"Selected scaling method\"] = scaling_method\n",
        "    df_comp[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    def color_cmp(row):\n",
        "        metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\"]\n",
        "        styles = []\n",
        "        for m in metrics:\n",
        "            dval = row[f\"{m} (Default)\"]\n",
        "            tval = row[f\"{m} (Tuned)\"]\n",
        "            styles.append(\"\")\n",
        "            if tval > dval:\n",
        "                styles.append(\"color: green\")\n",
        "            elif tval < dval:\n",
        "                styles.append(\"color: red\")\n",
        "            else:\n",
        "                styles.append(\"color: black\")\n",
        "        styles.extend([\"\", \"\", \"\"])\n",
        "        return styles\n",
        "    styled = df_comp.style.apply(color_cmp, axis=1)\n",
        "    return df_comp, styled\n",
        "\n",
        "def append_results_to_csv(df_comp, fname):\n",
        "    if not os.path.exists(fname):\n",
        "        df_comp.to_csv(fname, index=True)\n",
        "    else:\n",
        "        df_comp.to_csv(fname, mode='a', header=False, index=True)\n",
        "\n",
        "def plot_roc_and_confusion(default_models, tuned_models, X_test, y_test):\n",
        "    if not default_models:\n",
        "        print(\"No models => skipping combined plot.\")\n",
        "        return\n",
        "    from tqdm.notebook import tqdm\n",
        "    mnames = sorted(default_models.keys())\n",
        "    n_models = len(default_models)\n",
        "    fig, axes = plt.subplots(nrows=n_models, ncols=3, figsize=(12, 4 * n_models))\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    print(\"Plotting combined ROC + confusion for each model in 1 figure...\")\n",
        "    with tqdm(total=n_models, desc=\"ROC/Confusion Plots\") as pbar:\n",
        "        for i, mname in enumerate(mnames):\n",
        "            ax_roc = axes[i][0]\n",
        "            ax_def = axes[i][1]\n",
        "            ax_tun = axes[i][2]\n",
        "            dmodel = default_models[mname]\n",
        "            tmodel = tuned_models[mname]\n",
        "            if hasattr(dmodel, \"predict_proba\"):\n",
        "                y_prob_def = dmodel.predict_proba(X_test)[:, 1]\n",
        "                auc_d = roc_auc_score(y_test, y_prob_def)\n",
        "                fpr_d, tpr_d, _ = roc_curve(y_test, y_prob_def)\n",
        "            else:\n",
        "                if hasattr(dmodel, \"decision_function\"):\n",
        "                    dec = dmodel.decision_function(X_test)\n",
        "                    dmin = dec.min()\n",
        "                    dmax = dec.max()\n",
        "                    y_prob_def = (dec - dmin) / (dmax - dmin) if dmax != dmin else np.zeros_like(dec)\n",
        "                    auc_d = roc_auc_score(y_test, y_prob_def)\n",
        "                    fpr_d, tpr_d, _ = roc_curve(y_test, y_prob_def)\n",
        "                else:\n",
        "                    auc_d = -1.0\n",
        "                    fpr_d, tpr_d = [0, 1], [0, 1]\n",
        "            if hasattr(tmodel, \"predict_proba\"):\n",
        "                y_prob_tun = tmodel.predict_proba(X_test)[:, 1]\n",
        "                auc_t = roc_auc_score(y_test, y_prob_tun)\n",
        "                fpr_t, tpr_t, _ = roc_curve(y_test, y_prob_tun)\n",
        "            else:\n",
        "                if hasattr(tmodel, \"decision_function\"):\n",
        "                    dec_t = tmodel.decision_function(X_test)\n",
        "                    dmin = dec_t.min()\n",
        "                    dmax = dec_t.max()\n",
        "                    y_prob_tun = (dec_t - dmin) / (dmax - dmin) if dmax != dmin else np.zeros_like(dec_t)\n",
        "                    auc_t = roc_auc_score(y_test, y_prob_tun)\n",
        "                    fpr_t, tpr_t, _ = roc_curve(y_test, y_prob_tun)\n",
        "                else:\n",
        "                    auc_t = -1.0\n",
        "                    fpr_t, tpr_t = [0, 1], [0, 1]\n",
        "            ax_roc.plot(fpr_d, tpr_d, color=\"gray\", lw=2, label=f\"Default AUC={auc_d:.4f}\")\n",
        "            ax_roc.plot(fpr_t, tpr_t, color=\"red\", lw=2, linestyle=\":\", label=f\"Tuned AUC={auc_t:.4f}\")\n",
        "            ax_roc.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "            ax_roc.set_title(f\"{mname}\\nROC Curve\")\n",
        "            ax_roc.set_xlabel(\"False Positive Rate\")\n",
        "            ax_roc.set_ylabel(\"True Positive Rate\")\n",
        "            ax_roc.legend(loc=\"lower right\")\n",
        "            ax_roc.grid(True)\n",
        "            ax_roc.set_aspect(\"equal\", \"box\")\n",
        "            cm_def = confusion_matrix(y_test, dmodel.predict(X_test))\n",
        "            _plot_small_cm(ax_def, cm_def, label_text=True)\n",
        "            ax_def.set_title(\"ConfMatrix\\n(Default)\")\n",
        "            cm_tun = confusion_matrix(y_test, tmodel.predict(X_test))\n",
        "            _plot_small_cm(ax_tun, cm_tun, label_text=True)\n",
        "            ax_tun.set_title(\"ConfMatrix\\n(Tuned)\")\n",
        "            pbar.update(1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_small_cm(ax, cm, label_text=False):\n",
        "    cell_size = 0.2\n",
        "    colors = np.array([\n",
        "        [\"#a5e8a3\", \"#ffaaaa\"],\n",
        "        [\"#ffaaaa\", \"#a5e8a3\"]\n",
        "    ])\n",
        "    cell_labels = [\n",
        "        [\"TN\", \"FP\"],\n",
        "        [\"FN\", \"TP\"]\n",
        "    ]\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.add_patch(plt.Rectangle((j * cell_size, i * cell_size), cell_size, cell_size,\n",
        "                                       facecolor=colors[i, j], edgecolor='none'))\n",
        "            txt = f\"{cell_labels[i][j]}={cm[i, j]}\" if label_text else str(cm[i, j])\n",
        "            ax.text(j * cell_size + cell_size / 2, i * cell_size + cell_size / 2, txt,\n",
        "                    ha='center', va='center', fontsize=8, color='black')\n",
        "    ax.set_xlim(0, 2 * cell_size)\n",
        "    ax.set_ylim(0, 2 * cell_size)\n",
        "    ax.set_xticks([cell_size / 2, cell_size * 1.5])\n",
        "    ax.set_xticklabels([\"Neg\", \"Pos\"], fontsize=8)\n",
        "    ax.set_yticks([cell_size / 2, cell_size * 1.5])\n",
        "    ax.set_yticklabels([\"Neg\", \"Pos\"], fontsize=8)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_frame_on(False)\n",
        "    for s in ax.spines.values():\n",
        "        s.set_visible(False)\n",
        "\n",
        "###############################################################################\n",
        "# UI + Execution\n",
        "###############################################################################\n",
        "data_source_title = widgets.HTML(value=\"<b>Select data source:</b>\")\n",
        "file_path_title   = widgets.HTML(value=\"<b>CSV File Path (if 'file' selected):</b>\")\n",
        "search_title      = widgets.HTML(value=\"<b>Selected search method:</b>\")\n",
        "scaling_title     = widgets.HTML(value=\"<b>Selected scaling method:</b>\")\n",
        "models_title      = widgets.HTML(value=\"<b>Selected models:</b>\")\n",
        "max_cols_title    = widgets.HTML(value=\"<b>Max numeric columns for pairplot:</b>\")\n",
        "\n",
        "# Default to \"file\" as requested.\n",
        "data_source_widget = widgets.RadioButtons(\n",
        "    options=[\"heart_disease (embedded)\", \"breast_cancer (embedded)\", \"file\"],\n",
        "    value=\"file\"\n",
        ")\n",
        "file_path_widget = widgets.Text(value=\"breast_cancer_dataset_edit.csv\")\n",
        "# Set search method default to 'grid'\n",
        "search_method_widget = widgets.RadioButtons(\n",
        "    options=['grid', 'random', 'bayesian', 'none'],\n",
        "    value='grid'\n",
        ")\n",
        "scaling_method_widget = widgets.RadioButtons(\n",
        "    options=[\n",
        "        'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'MaxAbsScaler',\n",
        "        'Normalizer', 'QuantileTransformer', 'PowerTransformer', 'none'\n",
        "    ],\n",
        "    value='StandardScaler'\n",
        ")\n",
        "\n",
        "# New checkboxes to control log detail:\n",
        "default_log_checkbox = widgets.Checkbox(value=True, description=\"Show Default Training log\")\n",
        "tuning_log_checkbox = widgets.Checkbox(value=True, description=\"Show Hyperparameter Tuning log\")\n",
        "\n",
        "model_options = [\n",
        "    'Naive Bayes',\n",
        "    'k-nearest neighbors algorithm',\n",
        "    'Logistic Regression',\n",
        "    'Ridge Classifier',\n",
        "    'Linear Discriminant Analysis',\n",
        "    'Quadratic Discriminant Analysis',\n",
        "    'Decision Tree',\n",
        "    'Support vector machine',\n",
        "    'Random Forest',\n",
        "    'Extra Trees',\n",
        "    'AdaBoost',\n",
        "    'Gradient Boosting',\n",
        "    'eXtreme Gradient Boosting',\n",
        "    'LightGBM',\n",
        "    'CatBoost',\n",
        "    'Stacked Generalization',\n",
        "    'Gaussian Process'\n",
        "]\n",
        "model_checkboxes = [widgets.Checkbox(value=True, description=m) for m in model_options]\n",
        "model_selection_box = widgets.VBox(model_checkboxes)\n",
        "\n",
        "max_cols_widget = widgets.IntSlider(value=3, min=1, max=50, step=1)\n",
        "run_button = widgets.Button(description=\"Run Analysis\")\n",
        "output = widgets.Output()\n",
        "\n",
        "display(data_source_title, data_source_widget)\n",
        "display(file_path_title, file_path_widget)\n",
        "display(search_title, search_method_widget)\n",
        "display(scaling_title, scaling_method_widget)\n",
        "display(models_title, model_selection_box)\n",
        "display(max_cols_title, max_cols_widget)\n",
        "display(default_log_checkbox, tuning_log_checkbox)\n",
        "display(run_button, output)\n",
        "\n",
        "def on_button_clicked(_):\n",
        "    run_button.disabled = True\n",
        "    data_source_widget.disabled = True\n",
        "    file_path_widget.disabled = True\n",
        "    search_method_widget.disabled = True\n",
        "    scaling_method_widget.disabled = True\n",
        "    max_cols_widget.disabled = True\n",
        "    default_log_checkbox.disabled = True\n",
        "    tuning_log_checkbox.disabled = True\n",
        "    for cb in model_checkboxes:\n",
        "        cb.disabled = True\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "        data_source = data_source_widget.value\n",
        "        file_path = file_path_widget.value\n",
        "        max_cols = max_cols_widget.value\n",
        "        search_meth = search_method_widget.value\n",
        "        scale_meth = scaling_method_widget.value\n",
        "        selected_ms = [cb.description for cb in model_checkboxes if cb.value]\n",
        "        show_default_log = default_log_checkbox.value\n",
        "        show_tuning_log = tuning_log_checkbox.value\n",
        "\n",
        "        print(f\"Data Source: {data_source}\")\n",
        "        if data_source == \"file\":\n",
        "            print(f\"CSV File Path: {file_path}\")\n",
        "        print(f\"Max numeric columns for pairplot: {max_cols}\")\n",
        "        print(f\"Selected search method: {search_meth}\")\n",
        "        print(f\"Selected scaling method: {scale_meth}\")\n",
        "        print(f\"Selected models: {selected_ms}\\n\")\n",
        "\n",
        "        if data_source == \"heart_disease (embedded)\":\n",
        "            result_file = \"heart_disease_result.csv\"\n",
        "            df = pd.read_csv(io.StringIO(HEART_DISEASE_CSV_DATA))\n",
        "        elif data_source == \"breast_cancer (embedded)\":\n",
        "            result_file = \"breast_cancer_result.csv\"\n",
        "            df = pd.read_csv(io.StringIO(BREAST_CANCER_CSV_DATA))\n",
        "        else:\n",
        "            result_file = file_path.rstrip().replace(\".csv\", \"_result.csv\")\n",
        "            df = load_file_data(file_path)\n",
        "\n",
        "        dynamic_explore_data(df, max_cols)\n",
        "\n",
        "        if \"target\" not in df.columns:\n",
        "            print(\"Error: 'target' col not found => stopping pipeline.\")\n",
        "            return\n",
        "\n",
        "        X = df.drop(\"target\", axis=1)\n",
        "        y = df[\"target\"]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, stratify=y, random_state=42\n",
        "        )\n",
        "\n",
        "        all_models = define_all_models(scale_meth)\n",
        "        default_models = {m: all_models[m] for m in selected_ms}\n",
        "\n",
        "        print(\"\\n=== Default Training ===\")\n",
        "        default_results = {}\n",
        "        with tqdm(total=len(default_models), desc=\"Default Training\") as pbar_def:\n",
        "            for mname, model in default_models.items():\n",
        "                model.fit(X_train, y_train)\n",
        "                res = evaluate_model(model, X_test, y_test)\n",
        "                default_results[mname] = res\n",
        "                print(f\" {mname}: AUC={res['AUC']:.4f}\")\n",
        "                if show_default_log:\n",
        "                    print(f\"   Default Pipeline Params: {model.get_params()}\\n\")\n",
        "                pbar_def.update(1)\n",
        "\n",
        "        param_grids = define_param_grids()\n",
        "        tuned_models, tuned_results, best_params = tune_models(\n",
        "            default_models, param_grids, X_train, y_train, X_test, y_test, search_meth, show_tuning_log\n",
        "        )\n",
        "\n",
        "        df_comp, styled_comp = build_comparison_table(default_results, tuned_results, search_meth, scale_meth)\n",
        "        print(\"\\nComparison of Default vs. Tuned:\")\n",
        "        display(styled_comp)\n",
        "\n",
        "        append_results_to_csv(df_comp, result_file)\n",
        "        print(f\"\\nResults appended to '{result_file}'.\\n\")\n",
        "\n",
        "        best_tuned_auc = -np.inf\n",
        "        best_tuned_name = None\n",
        "        for nm, mets in tuned_results.items():\n",
        "            if mets[\"AUC\"] > best_tuned_auc:\n",
        "                best_tuned_auc = mets[\"AUC\"]\n",
        "                best_tuned_name = nm\n",
        "        if best_tuned_name:\n",
        "            print(\"\\nHighest AUC (Tuned):\", best_tuned_name, best_tuned_auc)\n",
        "\n",
        "        best_def_auc = -np.inf\n",
        "        best_def_name = None\n",
        "        for nm, mets in default_results.items():\n",
        "            if mets[\"AUC\"] > best_def_auc:\n",
        "                best_def_auc = mets[\"AUC\"]\n",
        "                best_def_name = nm\n",
        "        if best_def_name:\n",
        "            print(\"\\nHighest AUC (Default):\", best_def_name, best_def_auc)\n",
        "\n",
        "        print(\"\\nPlotting combined ROC + Confusions:\")\n",
        "        plot_roc_and_confusion(default_models, tuned_models, X_test, y_test)\n",
        "\n",
        "    run_button.disabled = False\n",
        "    data_source_widget.disabled = False\n",
        "    file_path_widget.disabled = False\n",
        "    search_method_widget.disabled = False\n",
        "    scaling_method_widget.disabled = False\n",
        "    max_cols_widget.disabled = False\n",
        "    default_log_checkbox.disabled = False\n",
        "    tuning_log_checkbox.disabled = False\n",
        "    for cb in model_checkboxes:\n",
        "        cb.disabled = False\n",
        "\n",
        "run_button.on_click(on_button_clicked)\n"
      ],
      "metadata": {
        "id": "j6G4qIJXg8aI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}